{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Data Crawling\n",
    "In this notebook, we will be crawling data from Facebook using the Facebook Graph API. We will be using the facebook-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the required library\n",
    "We will be using the facebook-scraper library to crawl data from Facebook. We will install this library using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting facebook_scraper\n",
      "  Using cached facebook_scraper-0.2.59-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.10/site-packages (1.26.2)\n",
      "Collecting ast\n",
      "  Using cached AST-0.0.2.tar.gz (19 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[19 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/codespace/.python/current/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/codespace/.python/current/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m   File \"/home/codespace/.python/current/lib/python3.10/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-d6iezzh7/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 325, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._get_build_requires(config_settings, requirements=['wheel'])\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-d6iezzh7/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 295, in _get_build_requires\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-d6iezzh7/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 480, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super(_BuildMetaLegacyBackend, self).run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-d6iezzh7/overlay/lib/python3.10/site-packages/setuptools/build_meta.py\", line 311, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 6, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/home/codespace/.python/current/lib/python3.10/codecs.py\", line 906, in open\n",
      "  \u001b[31m   \u001b[0m     file = builtins.open(filename, mode, buffering)\n",
      "  \u001b[31m   \u001b[0m FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pip-install-2owoax8j/ast_2850ad67f25d4619ad21da4557784383/AST/README'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install facebook_scraper pandas numpy ast copy datetime re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from facebook_scraper import get_posts, get_profile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import copy\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl the data using facebook_scraper\n",
    "Now we can get the data from Facebook using the facebook_scraper library. We will be using the get_posts function to get the posts from the fanpage. This function will return a list of dictionaries, where each dictionary represents a post. We will be saving this list of dictionaries to a json file. More information about what you can do with the facebook_scraper library can be found here: https://github.com/kevinzg/facebook-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables\n",
    "First we have to define some variables that we will be using throughout the notebook. \n",
    "- FANPAGE_LINK: The link to the fanpage that we want to crawl data from. This can be found by going to the fanpage and copying the link from the address bar. For example, the link to the fanpage of the [Nintendo Switch](https://www.facebook.com/NintendoSwitch/) is https://www.facebook.com/NintendoSwitch/. We will be using this link as the value for FANPAGE_LINK.\n",
    "\n",
    "- COOKIE_PATH: The path to the cookie file that we will be using to authenticate with Facebook. This cookie file can be obtained by logging into Facebook and copying the cookie from the browser. For example, in Chromium, use extension [Get cookies.txt LOCALLY](https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid) to get the cookie file. Then save the cookie to a file and use the path to this file as the value for COOKIE_PATH. <span style=\"color:red; font-weight:bold\">USE COOKIE FROM A FAKE ACCOUNT, OTHERWISE YOUR REAL ACCOUNT MIGHT GET BANNED.</span>.\n",
    "\n",
    "\n",
    "- FOLDER_NAME: The name of the folder that we will be saving the data to. This folder will be created in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FANPAGE_LINK =\"EliudKipchogeOfficial\"\n",
    "FOLDER_PATH = \"Data/\"\n",
    "COOKIE_PATH = \"cookies.txt\"\n",
    "\n",
    "PAGES_NUMBER = 30 # Number of pages to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_list = []\n",
    "for post in get_posts(FANPAGE_LINK,\n",
    "                    options={\"comments\": True, \"reactions\": True, \"allow_extra_requests\": True},\n",
    "                    extra_info=True, pages=PAGES_NUMBER, cookies=COOKIE_PATH):\n",
    "    print(post)\n",
    "    post_list.append(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert list of dicts to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert the list of dictionaries to a pandas dataframe. We will be using the pandas library to do this. We will also be saving the dataframe to a xlxs or csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only for initial scraping of website\n",
    "\n",
    "# # Initialize dataframe to scrape Facebook post\n",
    "# post_df_full = pd.DataFrame(columns=post_list[0].keys(), index=range(len(post_list)), data=post_list)\n",
    "\n",
    "# # To df\n",
    "# path=FOLDER_PATH + FANPAGE_LINK + \"copy\" + \".csv\"\n",
    "# post_df_full.to_csv(path, index=False)\n",
    "# print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/EliudKipchogeOfficial.csv\n"
     ]
    }
   ],
   "source": [
    "# Used for subsequent sessions\n",
    "# import crawled data to dataframe\n",
    "path=FOLDER_PATH + FANPAGE_LINK + \".csv\"\n",
    "print(path)\n",
    "post_df_full = pd.read_csv(path, low_memory= False)\n",
    "LENGTH = len(post_df_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['post_id', 'text', 'shared_text', 'original_text', 'timestamp', 'likes', 'image', 'image_lowquality', 'images', 'images_lowquality', 'images_lowquality_description', 'video', 'video_duration_seconds', 'video_height', 'video_id', 'video_quality', 'video_size_MB', 'video_thumbnail', 'video_watches', 'video_width', 'post_url', 'link', 'links', 'user_id', 'username', 'user_url', 'is_live', 'factcheck', 'shared_post_id', 'shared_time', 'shared_user_id', 'shared_username', 'shared_post_url', 'available', 'w3_fb_url', 'with', 'page_id', 'sharers', 'image_id', 'image_ids', 'video_ids', 'videos', 'was_live', 'fetched_time']\n"
     ]
    }
   ],
   "source": [
    "# list of fields to be omitted from data file used for analysis\n",
    "file = open(\"fields.txt\",\"r\")\n",
    "words = list()\n",
    "for line in file:\n",
    "    wordie = copy.deepcopy(line.replace('\\n', ''))\n",
    "    words.append(wordie)\n",
    "print(words)\n",
    "\n",
    "# removing said fields from dataframe\n",
    "post_df_full = post_df_full.drop(words,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text of each post, removing icons, leaving only text, numbers and common punctuation\n",
    "post_df_full['post_text'] = post_df_full['post_text'].str.replace('[^a-zA-Z0-9,.\\s]', '', regex=True)\n",
    "post_df_full['text_length'] = post_df_full['post_text'].str.len()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing cells with no data to \"no data\"\n",
    "post_df_full = post_df_full.fillna({'reactions': \"no_data\", \"reactors\": \"no_data\"})\n",
    "post_df_full['comments_full'] = post_df_full[\"comments_full\"].replace({\"[]\": \"no_data\"})\n",
    "post_df_full['reactors'] = post_df_full[\"reactors\"].replace({\"[]\": \"no_data\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract number of each type of reaction \n",
    "emojis_count = {'like': [], 'love': [], 'haha': [], 'wow': [], 'care': [], 'angry': [], 'sad': []}\n",
    "emojis = set(emojis_count.keys())\n",
    "for i in range(LENGTH):\n",
    "    reacts = copy.deepcopy(emojis)\n",
    "    if post_df_full.loc[i, \"reactions\"] != \"no_data\":\n",
    "        react = ast.literal_eval(post_df_full.loc[i, \"reactions\"])\n",
    "        for i in react.keys():\n",
    "            emojis_count[i].append(react[i])\n",
    "            reacts.remove(i)\n",
    "        for i in reacts:\n",
    "            emojis_count[i].append(0)\n",
    "    else:\n",
    "        for i in reacts:\n",
    "            emojis_count[i].append(0)\n",
    "for emoji in emojis:\n",
    "    post_df_full[emoji] = emojis_count[emoji]\n",
    "post_df_full = post_df_full.drop([\"reactions\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find reactors' facebook accounts' links\n",
    "reactor_ids = list()\n",
    "for i in range(LENGTH):\n",
    "    if post_df_full.loc[i, \"reactors\"] != \"no_data\":\n",
    "        reactor_id = list() \n",
    "        reactors = ast.literal_eval(post_df_full.loc[i, \"reactors\"])\n",
    "        for person in reactors:\n",
    "            reactor_id.append(person['link'])\n",
    "        reactor_ids.append(copy.deepcopy(reactor_id))\n",
    "    else:\n",
    "        reactor_ids.append(\"no_data\")\n",
    "post_df_full['reactor_ids'] = reactor_ids\n",
    "post_df_full = post_df_full.drop([\"reactors\"], axis=1)\n",
    "\n",
    "# Find IDs of everyone who reacted\n",
    "reactors_ids = list()\n",
    "pattern = r'id=(\\d{15})'\n",
    "for ids in reactor_ids:\n",
    "    if ids != \"no_data\":\n",
    "        matches = list()\n",
    "        for id in ids:\n",
    "            x = re.findall(pattern, id)\n",
    "            if len(x) > 0:\n",
    "                matches.append(x[0])\n",
    "        reactors_ids.append(matches)\n",
    "    else:\n",
    "        reactors_ids.append(\"no_data\")\n",
    "        \n",
    "post_df_full['reactor_ids'] = reactors_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant information from comments: text and commenters' ids, find average comment length of each post\n",
    "comments_texts = []\n",
    "commenters_ids = []\n",
    "comment_avg_length = list()\n",
    "for i in range(LENGTH):\n",
    "    if post_df_full.loc[i, \"comments_full\"] != \"no_data\":\n",
    "        counter = 0\n",
    "        comment_length = list()\n",
    "        comment_text = \"\"\n",
    "        commenters_id = list()\n",
    "        commenters = eval(post_df_full.loc[i, \"comments_full\"])\n",
    "        for person in commenters:\n",
    "            commenters_id.append(person['commenter_id'])\n",
    "            comment_text += person['comment_text']\n",
    "            comment_text += \". \"\n",
    "            counter += 1\n",
    "            comment_length.append(len(person['comment_text']))\n",
    "        commenters_ids.append(copy.deepcopy(commenters_id))\n",
    "        comments_texts.append(comment_text)\n",
    "        comment_avg_length.append(sum(comment_length) / counter)\n",
    "    else:\n",
    "        commenters_ids.append(\"no_data\")\n",
    "        comments_texts.append(\"no_data\")\n",
    "        comment_avg_length.append(0)\n",
    "post_df_full[\"comments_text\"] = comments_texts\n",
    "post_df_full[\"comments_text\"] = post_df_full[\"comments_text\"].str.replace('[^a-zA-Z0-9,.\\s]', '', regex=True)\n",
    "post_df_full[\"comments_text\"] = post_df_full[\"comments_text\"].apply(lambda x: x.replace('\\n', ' '))\n",
    "post_df_full[\"commenters_ids\"] = commenters_ids\n",
    "post_df_full[\"avg_comment_length\"] = comment_avg_length\n",
    "post_df_full = post_df_full.drop([\"comments_full\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe as csv file\n",
    "post_df_full.to_csv(\"cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
